---
# This workaround is needed because the current release of Bitwarden Secret Manager is bugged and
# two variables cannot be used at the same time.
# https://community.bitwarden.com/t/bitwarden-lookup-plugin-for-ansible-causes-panick-errors-and-a-worker-was-found-in-a-dead-state/64579/7
- name: Bitwarden workaround for vpn_turn_password
  ansible.builtin.set_fact:
    vpn_turn_password: "{{ vpn_turn_password }}"

- name: Bitwarden workaround for vpn_porkbun_api_key
  ansible.builtin.set_fact:
    vpn_porkbun_api_key: "{{ vpn_porkbun_api_key }}"

- name: Bitwarden workaround for vpn_porkbun_secret_key
  ansible.builtin.set_fact:
    vpn_porkbun_secret_key: "{{ vpn_porkbun_secret_key }}"

- name: Bitwarden workaround for vpn_relay_secret
  ansible.builtin.set_fact:
    vpn_relay_secret: "{{ vpn_relay_secret }}"

- name: Bitwarden workaround for vpn_exit_node_setup_key
  ansible.builtin.set_fact:
    vpn_exit_node_setup_key: "{{ vpn_exit_node_setup_key }}"

- name: Add user {{ vpn_netbird_dashboard_container_user }}
  ansible.builtin.user:
    name: "{{ vpn_netbird_dashboard_container_user }}"
    comment: User used to run Netbird dashboard
    create_home: false
    shell: /usr/sbin/nologin
  register: vpn_netbird_dashboard_container_user_result

- name: Add user {{ vpn_netbird_mgmt_container_user }}
  ansible.builtin.user:
    name: "{{ vpn_netbird_mgmt_container_user }}"
    comment: User used to run Netbird mgmt
    create_home: false
    shell: /usr/sbin/nologin
  register: vpn_netbird_mgmt_container_user_result

- name: Add user {{ vpn_netbird_signal_container_user }}
  ansible.builtin.user:
    name: "{{ vpn_netbird_signal_container_user }}"
    comment: User used to run Netbird signal
    create_home: false
    shell: /usr/sbin/nologin
  register: vpn_netbird_signal_container_user_result

- name: Add user {{ vpn_netbird_coturn_container_user }}
  ansible.builtin.user:
    name: "{{ vpn_netbird_coturn_container_user }}"
    comment: User used to run Netbird coturn
    create_home: false
    shell: /usr/sbin/nologin
  register: vpn_netbird_coturn_container_user_result

- name: Add user {{ vpn_netbird_relay_container_user }}
  ansible.builtin.user:
    name: "{{ vpn_netbird_relay_container_user }}"
    comment: User used to run Netbird relay
    create_home: false
    shell: /usr/sbin/nologin
  register: vpn_netbird_relay_container_user_result

- name: Add user {{ vpn_netbird_exit_node_container_user }}
  ansible.builtin.user:
    name: "{{ vpn_netbird_exit_node_container_user }}"
    comment: User used to run Netbird exit node
    create_home: false
    shell: /usr/sbin/nologin
  register: vpn_netbird_exit_node_container_user_result

- name: Create Netbird dashboard directories
  ansible.builtin.file:
    path: "{{ item }}"
    state: directory
    mode: "0770"
    owner: "{{ vpn_netbird_dashboard_container_user }}"
    group: "{{ vpn_netbird_dashboard_container_user }}"
  with_items:
    - "{{ vpn_netbird_dashboard_conf_dir }}"

- name: Create Netbird mgmt directories
  ansible.builtin.file:
    path: "{{ item }}"
    state: directory
    mode: "0770"
    owner: "{{ vpn_netbird_mgmt_container_user }}"
    group: "{{ vpn_netbird_mgmt_container_user }}"
  with_items:
    - "{{ vpn_netbird_mgmt_conf_dir }}"
    - "{{ vpn_netbird_mgmt_work_dir }}"

- name: Create Netbird signal directories
  ansible.builtin.file:
    path: "{{ item }}"
    state: directory
    mode: "0770"
    owner: "{{ vpn_netbird_signal_container_user }}"
    group: "{{ vpn_netbird_signal_container_user }}"
  with_items:
    - "{{ vpn_netbird_signal_work_dir }}"

- name: Create Netbird coturn directories
  ansible.builtin.file:
    path: "{{ item }}"
    state: directory
    mode: "0770"
    owner: "{{ vpn_netbird_coturn_container_user }}"
    group: "{{ vpn_netbird_coturn_container_user }}"
  with_items:
    - "{{ vpn_netbird_coturn_conf_dir }}"

- name: Create Netbird relay directories
  ansible.builtin.file:
    path: "{{ item }}"
    state: directory
    mode: "0770"
    owner: "{{ vpn_netbird_relay_container_user }}"
    group: "{{ vpn_netbird_relay_container_user }}"
  with_items:
    - "{{ vpn_netbird_relay_conf_dir }}"

- name: Create Netbird exit node directories
  ansible.builtin.file:
    path: "{{ item }}"
    state: directory
    mode: "0770"
    owner: "{{ vpn_netbird_exit_node_container_user }}"
    group: "{{ vpn_netbird_exit_node_container_user }}"
  with_items:
    - "{{ vpn_netbird_exit_node_conf_dir }}"

- name: Create Netbird network Quadlet
  containers.podman.podman_network:
    name: netbird
    state: quadlet
    # Enable DNS resolution based on container names
    disable_dns: false
    dns:
      - "{{ internal_gateway }}"
    driver: bridge
    force: true
    recreate: true
    # Need to be reachable by Traefik
    internal: false
    interface_name: podman-netbird
    opt:
      isolate: true
  notify:
    - Reload systemd daemons for vpn role
    - Restart netbird network

- name: Trigger Handlers for netbird network
  ansible.builtin.meta: flush_handlers

- name: Start netbird network
  ansible.builtin.systemd_service:
    # The name is the name of the network + network suffix
    name: netbird-network
    state: started
    enabled: true

- name: Copy netbird-dashboard.env file
  ansible.builtin.template:
    src: netbird-dashboard.env.j2
    dest: "{{ vpn_netbird_dashboard_conf_dir }}/netbird-dashboard.env"
    mode: "0600"
  notify:
    - Restart netbird-dashboard service

- name: Create Netbird dashboard Quadlet
  containers.podman.podman_container:
    name: netbird-dashboard
    state: quadlet
    image: netbirdio/dashboard:{{ vpn_netbird_dashboard_version }}
    env_file: "{{ vpn_netbird_dashboard_conf_dir }}/netbird-dashboard.env"
    recreate: true
    network: netbird

    uidmap:
      - 0:{{ vpn_netbird_dashboard_container_user_result.uid }}:1
      - 1:4000000000:100000

    gidmap:
      - 0:{{ vpn_netbird_dashboard_container_user_result.group }}:1
      - 1:4000000000:100000

    labels:
      traefik.enable: true
      traefik.http.routers.netbird.rule: Host(`netbird.{{ internal_domain }}`)
      traefik.http.routers.netbird.entrypoints: websecure,public-secure
      traefik.http.routers.netbird.tls.certresolver: letsencrypt
      traefik.http.routers.netbird.tls.domains[0].main: netbird.{{ internal_domain }}
      traefik.http.services.netbird.loadbalancer.server.port: 80

    quadlet_options:
      - |
        [Install]
        WantedBy=default.target
        [Unit]
        StartLimitIntervalSec=0
        [Service]
        Restart=always
        RestartSec=60s
  notify:
    - Reload systemd daemons for vpn role
    - Restart netbird-dashboard service

- name: Trigger Handlers for netbird-dashboard
  ansible.builtin.meta: flush_handlers

- name: Start netbird-dashboard
  ansible.builtin.systemd_service:
    name: netbird-dashboard
    state: started
    enabled: true

- name: Copy netbird-mgmt.env file
  ansible.builtin.template:
    src: netbird-mgmt.env.j2
    dest: "{{ vpn_netbird_mgmt_conf_dir }}/netbird-mgmt.env"
    mode: "0600"
  notify:
    - Restart netbird-mgmt service

- name: Copy netbird-mgmt.json file
  ansible.builtin.template:
    src: netbird-mgmt.json.j2
    dest: "{{ vpn_netbird_mgmt_conf_dir }}/netbird-mgmt.json"
    mode: "0600"
    owner: "{{ vpn_netbird_mgmt_container_user }}"
    group: "{{ vpn_netbird_mgmt_container_user }}"
  notify:
    - Restart netbird-mgmt service

- name: Create Netbird mgmt Quadlet
  containers.podman.podman_container:
    name: netbird-mgmt
    state: quadlet
    image: netbirdio/management:{{ vpn_netbird_mgmt_version }}
    command: --port 8080 --log-file console --log-level info --disable-anonymous-metrics=true --single-account-mode-domain=netbird.{{ internal_domain }} --dns-domain=netbird.selfhosted
    env_file: "{{ vpn_netbird_mgmt_conf_dir }}/netbird-mgmt.env"
    recreate: true
    network: netbird

    volumes:
      - "{{ vpn_netbird_mgmt_work_dir }}:/var/lib/netbird"
      - "{{ vpn_netbird_mgmt_conf_dir }}/netbird-mgmt.json:/etc/netbird/management.json"

    uidmap:
      - 0:{{ vpn_netbird_mgmt_container_user_result.uid }}:1
      - 1:4000000000:100000

    gidmap:
      - 0:{{ vpn_netbird_mgmt_container_user_result.group }}:1
      - 1:4000000000:100000

    labels:
      traefik.enable: true
      traefik.http.routers.netbird-api.rule: '"Host(`netbird.{{ internal_domain }}`) && PathPrefix(`/api`)"'
      traefik.http.routers.netbird-api.service: netbird-api
      traefik.http.services.netbird-api.loadbalancer.server.port: 8080
      traefik.http.routers.netbird-api.entrypoints: websecure,public-secure
      traefik.http.routers.netbird-api.tls.certresolver: letsencrypt
      traefik.http.routers.netbird-management.rule: '"Host(`netbird.{{ internal_domain }}`) && PathPrefix(`/management.ManagementService/`)"'
      traefik.http.routers.netbird-management.service: netbird-management
      traefik.http.services.netbird-management.loadbalancer.server.port: 8080
      # gRPC scheme
      traefik.http.services.netbird-management.loadbalancer.server.scheme: h2c
      traefik.http.routers.netbird-management.entrypoints: websecure,public-secure
      traefik.http.routers.netbird-management.tls.certresolver: letsencrypt

    quadlet_options:
      - |
        [Install]
        WantedBy=default.target
        [Unit]
        Requires=netbird-dashboard.service
        StartLimitIntervalSec=0
        [Service]
        Restart=always
        RestartSec=60s
  notify:
    - Reload systemd daemons for vpn role
    - Restart netbird-mgmt service

- name: Trigger Handlers for netbird-mgmt
  ansible.builtin.meta: flush_handlers

- name: Start netbird-mgmt
  ansible.builtin.systemd_service:
    name: netbird-mgmt
    state: started
    enabled: true

- name: Create Netbird signal Quadlet
  containers.podman.podman_container:
    name: netbird-signal
    state: quadlet
    image: netbirdio/signal:{{ vpn_netbird_signal_version }}
    recreate: true
    network: netbird

    volumes:
      - "{{ vpn_netbird_signal_work_dir }}:/var/lib/netbird"

    uidmap:
      - 0:{{ vpn_netbird_signal_container_user_result.uid }}:1
      - 1:4000000000:100000

    gidmap:
      - 0:{{ vpn_netbird_signal_container_user_result.group }}:1
      - 1:4000000000:100000

    labels:
      traefik.enable: true
      traefik.http.routers.netbird-signal.rule: '"Host(`netbird.{{ internal_domain }}`) && PathPrefix(`/signalexchange.SignalExchange/`)"'
      traefik.http.routers.netbird-signal.entrypoints: websecure,public-secure
      traefik.http.routers.netbird-signal.tls.domains[0].main: netbird.{{ internal_domain }}
      traefik.http.services.netbird-signal.loadbalancer.server.port: 80
      # gRPC scheme
      traefik.http.services.netbird-signal.loadbalancer.server.scheme: h2c

    quadlet_options:
      - |
        [Install]
        WantedBy=default.target
        [Unit]
        StartLimitIntervalSec=0
        [Service]
        Restart=always
        RestartSec=60s
  notify:
    - Reload systemd daemons for vpn role
    - Restart netbird-signal service

- name: Trigger Handlers for netbird-signal
  ansible.builtin.meta: flush_handlers

- name: Start netbird-signal
  ansible.builtin.systemd_service:
    name: netbird-signal
    state: started
    enabled: true

- name: Copy netbird-turnserver.conf file
  ansible.builtin.template:
    src: netbird-turnserver.conf.j2
    dest: "{{ vpn_netbird_coturn_conf_dir }}/netbird-turnserver.conf"
    mode: "0600"
    owner: "{{ vpn_netbird_coturn_container_user }}"
    group: "{{ vpn_netbird_coturn_container_user }}"
  notify:
    - Restart netbird-coturn service

- name: Generate letsencrypt account key
  community.crypto.openssl_privatekey:
    path: "{{ vpn_netbird_coturn_conf_dir }}/netbird.{{ internal_domain }}.account.private.pem"

- name: Generate certificate private key
  community.crypto.openssl_privatekey:
    path: "{{ vpn_netbird_coturn_conf_dir }}/netbird.{{ internal_domain }}.private.pem"

- name: Generate OpenSSL Certificate Signing Request for netbird.{{ internal_domain }}
  community.crypto.openssl_csr:
    path: "{{ vpn_netbird_coturn_conf_dir }}/netbird.{{ internal_domain }}.csr"
    privatekey_path: "{{ vpn_netbird_coturn_conf_dir }}/netbird.{{ internal_domain }}.private.pem"
    common_name: netbird.{{ internal_domain }}

- name: Ensure letsencrypt account exists
  acme_account:
    account_key_src: "{{ vpn_netbird_coturn_conf_dir }}/netbird.{{ internal_domain }}.account.private.pem"
    state: present
    terms_agreed: yes
    acme_version: 2
    acme_directory: https://acme-v02.api.letsencrypt.org/directory
    contact:
      - "mailto:{{ acme_email }}"

- name: Create a challenge for netbird.{{ internal_domain }}
  acme_certificate:
    account_key_src: "{{ vpn_netbird_coturn_conf_dir }}/netbird.{{ internal_domain }}.account.private.pem"
    account_email: "{{ vpn_acme_email }}"
    src: "{{ vpn_netbird_coturn_conf_dir }}/netbird.{{ internal_domain }}.csr"
    cert: "{{ vpn_netbird_coturn_conf_dir }}/netbird.{{ internal_domain }}.crt"
    challenge: dns-01
    acme_directory: https://acme-v02.api.letsencrypt.org/directory
    acme_version: 2
    # Renew if the certificate is at least 30 days old
    remaining_days: 30
  register: netbird_coturn_challenge
  retries: 3
  delay: 20

- block:
  - name: Set dns challenge record name
    ansible.builtin.set_fact:
      dns_challenge_lookup_key: "netbird.{{ internal_domain }}"
      dns_challenge_record_name: "_acme-challenge.netbird"

  - name: Set dns challenge record value
    ansible.builtin.set_fact:
      dns_challenge_record_value: "{{ netbird_coturn_challenge.challenge_data[dns_challenge_lookup_key]['dns-01'].resource_value }}"

  # Register the record
  - name: Create DNS challenge record
    sdorra.porkbun.porkbun_record:
      state: present
      domain: "{{ internal_domain }}"
      record_type: TXT
      name: "{{ dns_challenge_record_name }}"
      content: "{{ dns_challenge_record_value }}"
      api_key: "{{ vpn_porkbun_api_key }}"
      secret_api_key: "{{ vpn_porkbun_secret_key }}"
      ttl: 0

  - name: Wait for DNS propagation
    ansible.builtin.wait_for:
      timeout: 300

  - name: Validate the challenge for netbird.{{ internal_domain }}
    acme_certificate:
      account_key_src: "{{ vpn_netbird_coturn_conf_dir }}/netbird.{{ internal_domain }}.account.private.pem"
      account_email: "{{ vpn_acme_email }}"
      src: "{{ vpn_netbird_coturn_conf_dir }}/netbird.{{ internal_domain }}.csr"
      cert: "{{ vpn_netbird_coturn_conf_dir }}/netbird.{{ internal_domain }}.crt"
      fullchain: "{{ vpn_netbird_coturn_conf_dir }}/netbird.{{ internal_domain }}.full-chain.crt"
      chain: "{{ vpn_netbird_coturn_conf_dir }}/netbird.{{ internal_domain }}.intermediate.crt"
      challenge: dns-01
      acme_directory: https://acme-v02.api.letsencrypt.org/directory
      acme_version: 2
      remaining_days: 30
      data: "{{ netbird_coturn_challenge }}"
    retries: 5
    delay: 60
    ignore_errors: true
    register: validation_result

  - name: Delete DNS challenge record
    sdorra.porkbun.porkbun_record:
      state: absent
      domain: "{{ internal_domain }}"
      record_type: TXT
      name: "{{ dns_challenge_record_name }}"
      content: "{{ dns_challenge_record_value }}"
      api_key: "{{ vpn_porkbun_api_key }}"
      secret_api_key: "{{ vpn_porkbun_secret_key }}"
      ttl: 0

  - name: letsencrypt certificate validation failed
    ansible.builtin.fail:
      msg: The validation of the certification failed
    when: validation_result is failed
  when: netbird_coturn_challenge is changed

- name: Change ownership of SSL artifacts
  ansible.builtin.file:
    path: "{{ item }}"
    owner: "{{ vpn_netbird_coturn_container_user }}"
    group: "{{ vpn_netbird_coturn_container_user }}"
  with_items:
    - "{{ vpn_netbird_coturn_conf_dir }}/netbird.{{ internal_domain }}.crt"
    - "{{ vpn_netbird_coturn_conf_dir }}/netbird.{{ internal_domain }}.private.pem"
  notify:
    - Restart netbird-coturn service

- name: Create Netbird coturn Quadlet
  containers.podman.podman_container:
    name: netbird-coturn
    state: quadlet
    image: coturn/coturn:{{ vpn_netbird_coturn_version }}
    command: -c /etc/turnserver.conf
    recreate: true
    network: host

    volumes:
      - "{{ vpn_netbird_coturn_conf_dir }}/netbird-turnserver.conf:/etc/turnserver.conf:ro"
      - "{{ vpn_netbird_coturn_conf_dir }}/netbird.{{ internal_domain }}.private.pem:/etc/coturn/private/privkey.pem:ro"
      - "{{ vpn_netbird_coturn_conf_dir }}/netbird.{{ internal_domain }}.crt:/etc/coturn/certs/cert.pem:ro"

    uidmap:
      - 0:4000000000:65534
      - 65534:{{ vpn_netbird_coturn_container_user_result.uid }}:1
      - 65535:4000065534:10000

    gidmap:
      - 0:4000000000:1
      - +65534:@{{ vpn_netbird_coturn_container_user_result.group }}:1

    quadlet_options:
      - |
        [Install]
        WantedBy=default.target
        [Unit]
        StartLimitIntervalSec=0
        [Service]
        Restart=always
        RestartSec=60s
  notify:
    - Reload systemd daemons for vpn role
    - Restart netbird-coturn service

- name: Trigger Handlers for netbird-coturn
  ansible.builtin.meta: flush_handlers

- name: Start netbird-coturn
  ansible.builtin.systemd_service:
    name: netbird-coturn
    state: started
    enabled: true

- name: Copy netbird-relay.env file
  ansible.builtin.template:
    src: netbird-relay.env.j2
    dest: "{{ vpn_netbird_relay_conf_dir }}/netbird-relay.env"
    mode: "0600"
  notify:
    - Restart netbird-relay service

- name: Create Netbird relay
  containers.podman.podman_container:
    name: netbird-relay
    state: quadlet
    image: netbirdio/relay:{{ vpn_netbird_relay_version }}
    env_file: "{{ vpn_netbird_relay_conf_dir }}/netbird-relay.env"
    recreate: true
    network: netbird

    uidmap:
      - 0:{{ vpn_netbird_relay_container_user_result.uid }}:1
      - 1:4000000000:10000

    gidmap:
      - 0:{{ vpn_netbird_relay_container_user_result.group }}:1
      - 1:4000000000:10000

    labels:
      traefik.enable: true
      traefik.http.routers.netbird-relay.rule: '"Host(`netbird.{{ internal_domain }}`) && PathPrefix(`/relay`)"'
      traefik.http.routers.netbird-relay.entrypoints: websecure,public-secure
      traefik.http.routers.netbird-relay.tls.domains[0].main: netbird.{{ internal_domain }}
      traefik.http.services.netbird-relay.loadbalancer.server.port: 8443

    quadlet_options:
      - |
        [Install]
        WantedBy=default.target
        [Unit]
        StartLimitIntervalSec=0
        [Service]
        Restart=always
        RestartSec=60s

  notify:
    - Reload systemd daemons for vpn role
    - Restart netbird-relay service

- name: Trigger Handlers for netbird-relay
  ansible.builtin.meta: flush_handlers

- name: Start netbird-relay
  ansible.builtin.systemd_service:
    name: netbird-relay
    state: started
    enabled: true

- name: Copy netbird-exit-node.env file
  ansible.builtin.template:
    src: netbird-exit-node.env.j2
    dest: "{{ vpn_netbird_exit_node_conf_dir }}/netbird-exit-node.env"
    mode: "0600"
  notify:
    - Restart netbird-exit-node service

- name: Copy netbird-exit-node-config.json file
  ansible.builtin.template:
    src: netbird-exit-node-config.json.j2
    dest: "{{ vpn_netbird_exit_node_conf_dir }}/netbird-exit-node-config.json"
    mode: "0600"
    owner: "{{ vpn_netbird_exit_node_container_user }}"
    group: "{{ vpn_netbird_exit_node_container_user }}"
  notify:
    - Restart netbird-exit-node service

- name: Create Netbird exit node
  containers.podman.podman_container:
    name: netbird-exit-node
    state: quadlet
    image: netbirdio/netbird:{{ vpn_netbird_client_version }}
    env_file: "{{ vpn_netbird_exit_node_conf_dir }}/netbird-exit-node.env"
    recreate: true
    network: netbird
    # The hostname is used by the Netbird client when registering in the mgmt plane
    hostname: netbird-exit-node

    cap_add:
      - "NET_ADMIN"
      - "SYS_ADMIN"
      - "SYS_RESOURCE"

    uidmap:
      - 0:{{ vpn_netbird_exit_node_container_user_result.uid }}:1
      - 1:4000000000:10000

    gidmap:
      - 0:{{ vpn_netbird_exit_node_container_user_result.group }}:1
      - 1:4000000000:10000

    quadlet_options:
      - |
        [Install]
        WantedBy=default.target
        [Unit]
        Requires=netbird-mgmt.service
        StartLimitIntervalSec=0
        [Service]
        Restart=always
        RestartSec=60s

  notify:
    - Reload systemd daemons for vpn role
    - Restart netbird-exit-node service

- name: Trigger Handlers for netbird-exit-node
  ansible.builtin.meta: flush_handlers

- name: Start netbird-exit-node
  ansible.builtin.systemd_service:
    name: netbird-exit-node
    state: started
    enabled: true
